---
title: "Predicting Loan Defaults with Logistic Regression"
author: "Josh Jarvey"
date: "6/5/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 4, fig.height = 4)
```

# 1.0 Executive Summary

Products within the banking industry are all typically based on a risk assessment, that is to say each offering comes with some sort of risk of "winning" or "losing" on that transaction. For instance, providing investment services to a consumer has an associated risk of increasing or decreasing in value based on numerous factors. When it comes to loans for home, auto, school, personal, etc. these products are no different - they have some associated risk of "paid in full" (with interest collected) or the loan will "default" - the win vs. loss metaphor mentioned earlier. 

Historically, the ability to assess risk on loans was something more of an art than a science. Financial bankers would meet with consumers in person, and while it's true that they would go over various details of finance and stability, the ultimate decision was grounded in intuition and "gut-feel" of that banker (in accordance to bank guidelines of course). Some were very successful at closing the right ones and making a profit for their organization, while others were not as successful. There's also the ethical dilemma of bias being introduced into gut-feel and intuitive decision making, which can lead to missed revenues, but also potential legal troubles that organizations are wise to mitigate.  

In today's age, the science has caught up with the art - and in many cases has even surpassed it. Where you have had one or two individuals trying to "crunch the numbers" and come up with a decision in the times past, we now have computers to do this work. But where the computers and science really excel at this work is finding the hidden relationships between a consumer's relevant "parameters" to the loan and aiding in the decision of win vs. loss. It crunches the numbers among thousands upon thousands of historical loan's parameters, and understanding those outcomes it can provide a sound assessment of risk that's rooted in science and not intuition.

In this project we will explore the use of one of these scientific tools called Regression. Being that our end goal is to have a decision to "give loan" or "decline loan", we will use the Logistic version of the regression analysis, which is better suited for a yes/no decision. A more scientific approach to decision making will not only save time and money to improve the bottom line, but will also help to mitigate bias and the associated organizational risk that comes with it.

\newpage

# 2.0 Introduction

In risk-based decision making, it can be hard for any one human to take into account all the necessary information and variables that go into making the *right* decisions that minimize losses and maximizes revenues. This is where Regression analysis can come into play, because it is designed specifically to tackle the analysis of many variables, at the same time, to predict an outcome. Since the end result of the product in this study is a successful financial loan, we know that the outcome will be one of two choices: either "paid in full" as a success, or "loan default" as a failure. Because of this binary in nature outcome, we will use the Logistic version of the Regression analysis, which is suited well for handling this type of task.

The remainder of this report will outline the steps taken to build, train, and test different candidate models to see which performs the best and will ultimately be released into production to serve its purpose of aiding in the loan decision making process.  

As with any data project, we start out by summarizing the dataset of 50,000 observations both numerically and visually to explore it for anomalies - missing data, outliers, etc. - and determine how to best overcome these data quality issues. In the next step we'll explore the dataset's 30 unique variables. Some of these variables will be significant toward solving the problem, whereas some will not and will need to be removed from the model or lumped together in an "other" category. Once we've landed on the appropriate variables, we'll take a look at the distribution of its values as data may need to be transformed depending on what is discovered.

In the second half of the project, we will describe fitting the model to the data and accessing its performance. From here we will test at optimizing for prediction accuracy in addition maximizing profit. Finally, we will end with a conclusion and summarize model performance, along with any room for enhancements.

# 3.0 Data Preparation and Cleaning

To get started in this step, first we will load the necessary packages we need in order to perform the data exploration and cleaning. 
```{r, message=FALSE, results='hide'}
lapply(c("readr","dplyr","car","mice","ggformula","ggpubr"), require, character.only = TRUE)
```

```{r, echo=FALSE, message=FALSE}
  #load the data
loans = read_csv("loans50k.csv")
```
Now prepare our response variable based on the status column of the existing dataset. We will discard any "in progress" loans, and only keep those that are "complete" - these three status reflect completed loans, leaving 34,655 observations remaining. 
```{r, message=FALSE}
loans = loans[loans$status %in% c("Fully Paid", "Charged Off", "Default"),]
```
Because our response variable must be binary in order to use logistic regression, we will condense the results into a 2-factor variable called "outcome".
```{r, message=FALSE}
loans = loans %>% mutate(outcome = as.factor(if_else(status == "Fully Paid","Good","Bad")))
```

## 3.1 Feature Selection

In an effort to keep parsimony, we will a look at each of these variables in the dataset and evaluate if we expect them to add overall value to the model. To get us started, we can drop the "status" variable since we used it already to create our new response variable.
```{r, message=FALSE}
loans = subset(loans, select = -status)
```

Below are a list of the other variables we'll remove from the dataset in the order of which they occur:

* "**loanID**": The id in which a loan can be retrieved from the database. This adds no information.
* "**employment**": A free form entry field that offers no consistency between values. Riddled with short hand and spelling mistakes.
* "**verified**": Whether the source of income has been verified. If the bank does not have this verified, we would not make the loan. 
* "**state**": With loans in an overwhelming majority of the states, this seems more noise than signal.
* "**debtIncRat**": Any ratios are a calculation of existing data within this set, so it will be removed since it provides no extra information.
* "**revolRatio**": Any ratios are a calculation of existing data within this set, so it will be removed since it provides no extra information.
* "**totalAcc**": Total accounts (including closed) - We're only interested in open accounts that are currently active. 
* "**totalPaid**": Total amount repaid to the bank. This comes after a loan is issued, so it cannot be used as a predictor. 
* "**bcRatio**": Any ratios are a calculation of existing data within this set, so it will be removed since it provides no extra information.\

```{r, echo=FALSE, message=FALSE}
loans = subset(loans, select = -c(loanID,employment,verified,state,debtIncRat,revolRatio,totalAcc,totalPaid,bcRatio))
```

## 3.2 Feature Engineering

Next we will take a look at the remaining variables with our str() function to check the variable types in our dataset. We see right away there are variables that are not coded appropriately (as a character, instead of as a factor), so we will make those adjustments now before moving forward.

```{r, echo=FALSE, message=FALSE, results='hide'}
str(loans)
```
```{r, message=FALSE}
loans = mutate_at(loans, vars(term,grade,length,home,reason), as.factor)
```

Now that we have the variable's types coded correctly, we can review the number and counts (i.e. general distribution) of categories within each of the converted variables using a barplot as a visual. 

```{r, echo=FALSE, fig.width = 7.5, fig.height = 4}
par(mfrow=c(2,3),mar=c(4,3,3,1))
barplot(table(loans$term), main = "Loan Term")
barplot(table(loans$grade), main = "Loan Grade")
barplot(table(loans$length), las=2, main = "Employment Length")
barplot(table(loans$home), main = "Living Type")
barplot(table(loans$reason), las=2, cex.names = 0.9, main = "Loan Reason")
```

First, we take a look at the loan term variable. While 36 month loans account for the vast majority, there are only two categories for this variable, so we wont make any adjustments on this one. 

Next, we take a look at the grade variable, which ranges in values from A to G. In reviewing the summary, we notice the F and G groups have a considerably less count than the other grades, so we'll combine these two groups into the "E" grade and form a new category "E or less".

```{r, message=FALSE}
loans$grade = recode(loans$grade,"c('E','F','G')='E or less'")
```

The employment length variable has many different categories, and the 10+ year is skewing the distribution, so let's combine into three categories as follows (we also have NA, and will deal with that in the next section):

* "0-4 years"
* "5-9 years"
* "10+ years"

```{r, echo=FALSE, message=FALSE, results='hide'}
loans$length = recode(loans$length,"c('< 1 year','1 year','2 years','3 years','4 years')='0-4 years';c('5 years','6 years','7 years','8 years','9 years')='5-9 years'")
```

We now review the home variable, which explains the type of living arrangement: rent vs. mortgage vs. owning. There are only three categories here, and the frequencies for each category look to be above 5% of the total number of observations, so we wont be making any adjustments here.

Finally, we have loan reason. Similar to the employment length variable, we will need to combine categories as some of these contain a very low number of observations. There also seems to be similar in nature categories such as "house" and "home improvement", so those will be a natural combination. The combined groups will be as follows:

* "**Home Expense**" = home_improvement, house, moving, renewable_energy
* "**Other**" = car, major_purchase, medical, other, small_business, vacation, wedding

```{r, echo=FALSE, message=FALSE, results='hide'}
loans$reason = recode(loans$reason,"'credit_card'='Credit Card';'debt_consolidation'='Debt Consolidation'; c('home_improvement','house','moving','renewable_energy')='Home Expense'; c('car','major_purchase','medical','other','small_business','vacation','wedding')='Other'")
```

## 3.2 Missing Data & Imputation

There are two variables that contain missing values - length (of employment) & bcOpen. Because "length" is a categorical variable, we won't be able to impute its value, so these 1,823 observations will be dropped.

```{r, message=FALSE}
loans = loans[which(loans$length != "n/a"),]
```

BcOpen, however, is a quantitative variable, therefore we can try an imputation process to determine "best fit" values for these missing data. We create a custom function impute_bcOpen() that takes in the original dataframe, performs the imputation process using the mice() package, and replaces the NA values with the imputed values. 

```{r, message=FALSE}
impute_bcOpen <- function(df, seed){
    #create a vector of the indices that are "NA"
  index_NA = which(is.na(df$bcOpen))
  
    #complete the imputation process
  imputation = mice(loans, seed = seed)

    #iterate through each missing value and calculate the average of the imputed set, replace NA with the imputed value.
  for(i in 1:length(index_NA)){
    sum = 0
    sum = sum + imputation$imp$bcOpen$`1`[i]
    sum = sum + imputation$imp$bcOpen$`2`[i]
    sum = sum + imputation$imp$bcOpen$`3`[i]
    sum = sum + imputation$imp$bcOpen$`4`[i]
    sum = sum + imputation$imp$bcOpen$`5`[i]
    df$bcOpen[index_NA[i]] = sum/5
  }
  return(df)
}
```

```{r, echo=FALSE, message=FALSE, results='hide'}
loans = impute_bcOpen(loans, seed = 123456)
```

# 4.0 Data Transformation

In preparation for model fitting, we need to take a look at the distribution of our quantitative variables to understand their shape. Should a distribution appear skewed in either direction, we can attempt a transformation to make it more "normal" in shape.

```{r, echo=FALSE, fig.width = 6.3, fig.height = 3}
par(mfrow=c(1,2),mar=c(4,4,4,1))
hist(loans$income, main = "Distribution of Income", xlab = "Dollars ($)")
hist(log(loans$income), main = "Distribution of log(Income)", xlab = "Dollars (Log $)")
par(mfrow=c(1,1))
loans$income = log(loans$income)
```

A prime example of skewed data is the "income" variable. In the first histogram we see a severely right-skewed distribution, so we attempt a log() transformation. The results of the log(income) show in the next graph - these are much more normally distributed, so we will keep this transformation on the "income" variable.

```{r,message=FALSE, echo=FALSE, include=FALSE, fig.width = 12, fig.height = 7}
  #set up 4 rows of 4 columns. Give enough margin spacing with mar()
par(mfrow=c(5,4),mar=c(4,4,4,1))
  
  #Line 1: amount & totalBal
hist(loans$amount, main = "amount", xlab = "")
hist(log(loans$amount), main = "log(amount)", xlab = "")
hist(loans$totalBal, main = "totalBal", xlab = "")
hist(log(loans$totalBal), main = "log(totalBal)", xlab = "")
  #Line 2: totalRevLim & bcOpen
hist(loans$totalRevLim, main = "totalRevLim", xlab = "")
hist(log(loans$totalRevLim), main = "log(totalRevLim)", xlab = "")
hist(loans$bcOpen, main = "bcOpen", xlab = "")
hist(log(loans$bcOpen), main = "log(bcOpen)", xlab = "")
  #Line 3: totalLim & total RevBal
hist(loans$totalLim, main = "totalLim", xlab = "")
hist(log(loans$totalLim), main = "log(totalLim)", xlab = "")
hist(loans$totalRevBal, main = "totalRevBal", xlab = "")
hist(log(loans$totalRevBal), main = "log(totalRevBal)", xlab = "")
  #Line 4: totalBcLim & totalILim
hist(loans$totalBcLim, main = "totalBcLim", xlab = "")
hist(log(loans$totalBcLim), main = "log(totalBcLim)", xlab = "")
hist(loans$totalIlLim, main = "totalIlLim", xlab = "")
hist(log(loans$totalIlLim), main = "log(totalIlLim)", xlab = "")
  #Line 5: avgBal
hist(loans$avgBal, main = "avgBal", xlab = "")
hist(log(loans$avgBal), main = "log(avgBal)", xlab = "")
  #reset my graphs back to a 1x1.
par(mfrow=c(1,1))
```


A look at the remaining dollars based variables suggest a log() transformation applied to each one with the exception of (loan) amount.

```{r,message=FALSE, echo=FALSE, include=FALSE}
  #creating a list of columns to transform
cols_to_transform = c("totalBal","totalRevLim","bcOpen","totalLim","totalRevBal","totalBcLim","totalIlLim","avgBal")
  #because log(0) = -inf, add +1 to the values first just to ensure this doesnt occur. It's by such a small factor that it wont be a large impact on the results.
loans[cols_to_transform] = loans[cols_to_transform]+1
  #applying the log transform to the columns.
loans[cols_to_transform] = lapply(loans[cols_to_transform], log)
rm(cols_to_transform)

```

## 4.1 Data Exploration

Our final step before model fitting is to explore some of the data within the variables to gain some intuition of distribution as it relates to the our outcome variable of "Good vs. Bad".

```{r,message=FALSE, echo=FALSE, fig.width = 8, fig.height = 3}
  #create a side by side box plot that shows the distribution of loan amounts by the outcome
bx_amount=gf_boxplot(amount~outcome, data=loans, color=~outcome, xlab="Loan Outcome", ylab="Loan Amount ($)", title='Distribution of Loan "Amount"')
  #create a side by side box plot that shows the distribution of loan amounts by the outcome
bx_rate=gf_boxplot(rate~outcome, data=loans, color=~outcome, xlab="Loan Outcome", ylab="Loan Rate (%)", title='Distribution of Loan "Rate"')
  #use ggarrange to set ggplots next to each other
ggarrange(bx_amount,bx_rate, ncol = 2, nrow = 1)
```

In the first boxplot we can see loan amounts (in $). Loans that have a "bad" outcome seem evenly distributed in their amounts, whereas "good" loans appear slightly skewed to the right. We perform a Wilcoxon Sum Rank test to check for significance between these distributions, and at the 95% significance level, there is enough evidence to conclude that the bad loan's median amount is larger than the good loan's median amount (p=2.2e-16).

```{r,message=FALSE, echo=FALSE}
wilcox.test(loans$amount~loans$outcome, alternative = "greater", conf.int = TRUE)
```
The second boxplot display's loan percentage rate distribution. Both data sets appear to be generally normally distributed with a few outliers, however, similar to the above, the "bad" loans appear to have a higher median. We perform a Wilcoxon Sum Rank test to check for significance between these distributions, and at the 95% significance level, there is enough evidence to conclude that the bad loan's median rate is larger than the good loan's median rate (p=2.2e-16).

```{r,message=FALSE, echo=FALSE}
wilcox.test(loans$rate~loans$outcome, alternative = "greater", conf.int = TRUE)
```

```{r,message=FALSE, echo=FALSE, fig.width = 8, fig.height = 3}
  #create a side by side bar chart to see the distribution of loan term by outcome. 
bar_grade=gf_bar(~grade, data = loans, fill = ~outcome, position = position_dodge(), title = 'Distribution of Loan "Grade"')
  #create a side by side bar chart to see the distribution of loan term by outcome. 
bar_term=gf_bar(~term, data = loans, fill = ~outcome, position = position_dodge(), title = 'Distribution of Loan "Term"')
  #use ggarrange to set ggplots next to each other
ggarrange(bar_grade, bar_term, ncol = 2, nrow = 1)
```
Now we can look at the first bar graph that shows the distribution of bad vs. good outcomes based on letter grade of the loan. We see some disproportion, with lesser graded loans having a higher number of bad outcomes. To test if the distribution is truly different among categories, we will perform a chi squared goodness of fit test. At a 95% significance level, there is enough evidence to support that these distributions of bad loans are different by loan grade (p=2.2e-16).
```{r,message=FALSE, echo=FALSE}
 #test if the number of "bad" loans is equal across each loan grade, or not equal to each other.
  #p1 = The proportion of bad outcomes for loan grade "A"
  #p2 = The proportion of bad outcomes for loan grade "B"
  #p3 = The proportion of bad outcomes for loan grade "C"
  #p4 = The proportion of bad outcomes for loan grade "D"
  #p5 = The proportion of bad outcomes for loan grade "E or less"
  #H0: p1 = p2 = p3 = p4 = p5
  #H_a: p1 <> p2 <> p3 <> p4 <> p5
  #alpha = 0.05

  #pull the counts of bad loans per each loan grade. 
bad_A = nrow(loans[loans$outcome == "Bad" & loans$grade == "A",])
bad_B = nrow(loans[loans$outcome == "Bad" & loans$grade == "B",])
bad_C = nrow(loans[loans$outcome == "Bad" & loans$grade == "C",])
bad_D = nrow(loans[loans$outcome == "Bad" & loans$grade == "D",])
bad_E = nrow(loans[loans$outcome == "Bad" & loans$grade == "E or less",])

  #create a matrix of grade, count, and equal proportions
grades = matrix(c("A",bad_A,0.20,"B",bad_B,0.20,"C",bad_C,0.20,"D",bad_D,0.20,"E or Less",bad_E,0.20), ncol = 3, byrow = TRUE)
colnames(grades) = c("Grade","Frequency of Bad","equaldistribution")

  #run the chi-squared goodness of fit test 
grades_test = chisq.test(as.numeric(grades[,2]), p = as.numeric(grades[,3]))
  #check expected values -> they are greater than 5 per category.
grades_test$expected
  #review results
grades_test
```

Finally, we review the proportions of bad vs. good outcomes for the 36 and 60 month loan terms. Visually, there appears to be a much lower proportion of bad loan outcomes in the 36 month term vs. the 60 month term, so we perform a proportion test. At a 95% significance level, there is enough evidence to support the proportion of bad loans in the 36 month term group is less than the proportion of bad loans in the 60 month term group (p=2.2e-16).

```{r,message=FALSE, echo=FALSE}
 
  #get the number of bad records from 36 month term
bad_36 = nrow(loans[loans$outcome == "Bad" & loans$term == "36 months",])
  #get the number of good records from 36 month term
good_36 = nrow(loans[loans$outcome == "Good" & loans$term == "36 months",])

  #get the number of bad records from 36 month term
bad_60 = nrow(loans[loans$outcome == "Bad" & loans$term == "60 months",])
  #get the number of good records from 36 month term
good_60 = nrow(loans[loans$outcome == "Good" & loans$term == "60 months",])


  #create a matrix of bad/good for 36/60 months
term = matrix(c(bad_36,good_36,bad_60,good_60), ncol = 2, byrow = TRUE)
rownames(term) = c("36 months","60 months")
colnames(term) = c("Bad","Good")

#data is prepped, so run prop.test to check if: proportion of bad loans in the 36month term < proportion of bad loans in the 60month term.
prop.test(term, alternative = "less", correct = TRUE)
```
