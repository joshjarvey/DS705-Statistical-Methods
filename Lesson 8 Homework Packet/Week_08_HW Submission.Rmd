
---
title: 'Regression Model Selection'
author: "Josh Jarvey"
date: "06/21/2020"
output: word_document
fontsize: 12pt
---

Knit a Word file from this R Markdown file for the following exercises.  Submit the R markdown file and resulting Word file.   

Be advised, this homework will produce copious amounts of output.

## Exercise 1

Ninety members (ages 18.1 to 23.4 years) of three Division I womenâ€™s intercollegiate rowing teams (National Collegiate Athletic Association) within the Big Ten Conference volunteered to participate in a study to predict race time for female collegiate rowers from nineteen physical characteristics.

Data is in the file rowtime.rda in the DS705data package.  The race times are in the variable named "racetime".

### Part 1a

Load the data and use summary(rowtime) to see a numerical summary of the values of each.  

(i) What type of variable is the response variable racetime (categorical or quantitative)?  

(ii) Does this indicate linear regression or logistic regression?

(iii) What types of variables are there in the pool of potential predictors? Categorical, quantitative, or a mixture of each?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #load the data and review a summary of the variables. 
library(DS705data)
data("rowtime")
summary(rowtime)
```

The response variable racetime is a quantitative variable because it is numerical and is continuous. Because this is a quantitative type, we will use a linear regression. As for the predictor variables, there is a mix of quantitative (tall, weight, armspan ,etc) as well as categorical: expvarsity (indicating if they have varsity experience?) and preexper (indicating if they have previous experience?).

### Part 1b

Use the **regsubsets** function to find the "best" first-order model for predicting the response variable racetime with up to 8 of the 19 predictor variables in the data set.  Produce the summary and the plot for the best single models with up to 8 predictors according to $R^2_{adj}$.

Which independent variables are in the best first-order model with 8 predictors when the $R^2_{adj}$ is the criterion for selection?

What is the $R^2_{adj}$ for the best first-order model?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1b -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #import the regsubsets() function from the leaps library
library(leaps)
  #using the "full" number of variables denoted by y~., step forward until you've determine the best model with 8 predictors.
    #nvmax is the number of variables to step toward. Obviously there may be increases or decreases to adjusted r2, so it might be best to allow it to go all the way to the max number of variables within the dataset and then just look at which one gives the best adjusted r2 (or whatever metric were interested in as the grading criteria).
allmodels=regsubsets(racetime~.,data = rowtime, nvmax = 8)
  #because "all" 8 of the models are stored, run summary to see the output. 
summary(allmodels)

  #the summary is ok, but this plot is where you can really see what model gives the best adjusted r2 measure. 
plot(allmodels, scale = "adjr2")

```

Based on the output of the regsubsets() function, I will choose a model with 6 independent variables: tall, calfcir, tricep, meso, expvarsity, preexper - with an adjusted R2 score of 0.60. I choose this model because in comparing the models with 7 or 8 variables, the adjusted R2 score only increased by 0.10 for each additional variable. This didn't seem like a fair trade off (add more complexity for minor gains in explainability), so i will settle with a slightly lesser adjusted R2 score for a simpler model.

### Part 1c

Use the **step** function with backward selection to find the "best" first-order model for predicting the response variable racetime.  Recall that the formula structure y~. will produce the model using y as the response variable and all other variables in the data set as the predictors; in this set "racetime" is the response (not y) and all other variables are potential predictors.

Which independent variables are in this model?  What is the AIC value for this model? 

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1c -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #start with a "full" model denoted by y~.
  #then do a backward step to remove variables from the equation until a best is reached. 
step(lm(racetime~., data=rowtime), direction = "backward")
```

The best model selected using a backward step algorithm has an AIC of 497.22, and includes the variables: tall, calfcir, biceps, estffm, bestvj, legpower, meso, expvarsity, & preexper. 
 
### Part 1d

Use the **step** function with forward selection to find the "best" model for predicting the response variable racetime.   Recall that the formula structure y~1 will produce the model using y as the response variable and no variables in the data set as the predictors, only an intercept.

Which independent variables are in the model selected?  What is the AIC value for this model? 

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1d -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #to begin the "forward" step function, we set a "null" model which is the y~1
nullmodel = lm(racetime~1, data = rowtime)
  #we set a "full" model to be the "cap" for the forward step function y~.
fullmodel = lm(racetime~., data = rowtime)
  #starting with the null model, add variables one at a time and test their impact, within the bounds of the lower and upper limits.
step(nullmodel,scope = list(lower=nullmodel, upper=fullmodel), direction = "forward")

```

The best model selected using a foward step algorithm has an AIC of 497.65, and includes the variables: estffm, expvarsity, tall, preexper, biceps, meso, calfcir, & bestvj.

### Part 1e

Use extractAIC to compute the AIC for the best first order-model with 8 predictors from the **regsubsets** function.  How does it compare with the AIC for the two models produced by the backward and forward selection procedure?

So far, which first-order model is the "best" according to the AIC?  (remember, smaller is better for AIC)

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1e -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #regsubsets w/8 variables
extractAIC(lm(formula = racetime~calfcir+biceps+bestvj+legpower+meso+ecto+expvarsity+preexper, data=rowtime))
  #step backward (9 variables)
extractAIC(lm(formula = racetime~tall+calfcir+biceps+estfm+bestvj+legpower+meso+expvarsity+preexper, data=rowtime))
  #step forward (8 variables)
extractAIC(lm(formula = racetime~estffm+expvarsity+tall+preexper+biceps+meso+calfcir+bestvj, data=rowtime))
```

The backward step() model containing 9 variables has the lowest AIC value of 497.2245.

### Part 1f

Find the VIF for each independent variable in the model produced by the forward step wise procedure.  Is there a serious problem with collinearity?  Explain.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1f -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #loading the car package for the vif() function
library(car)
  #checking the vif scores of the independent variables. greater than 10 suggests a collinearity problem.
vif(lm(formula = racetime~tall+calfcir+biceps+estfm+bestvj+legpower+meso+expvarsity+preexper, data=rowtime))
```

There does appear to be a collinearity problem with the "legpower" variable because the vif score of ~18.57 is greater than 10. 

### Part 1g

What about the possibility of adding quadratic terms to the model?  In this case, we have a fairly manageable number of quantitative predictors to check for quadratic relationship between the response variable racetime and any predictors.

The R function pairs() can be used to look for quadratic relationships, but it will have to be restricted to about 4 predictors at a time so that the scatterplot matrices will be legible.

Since the response variable is in column 1 and the quantitative predictors are in columns 2 through 18, running the R code in the chunk shown below.

In each plot scatterplot matrix that is produced, look for any quadratic relationships between racetime and any of the predictor by examining the plots in the first row.  Is there any obvious curvature in the trend for racetime with any of the predictors?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1g -|-|-|-|-|-|-|-|-|-|-|-

```{r}
# The code in this chunk is provided for students
library(DS705data)
data("rowtime")
pairs(rowtime[c(1,2,3,4)])
pairs(rowtime[c(1,5,6,7)])
pairs(rowtime[c(1,8,9,10)])
pairs(rowtime[c(1,11,12,13,14)])
pairs(rowtime[c(1,15,16,17,18)])
```

After studying these scatterplots of the dependent vs. independent variables for a while, I don't notice anything substantial that indicates a quadratic relationship. 

### Part 1h

Something new will be covered in this part.  All possible interactions can be examined using the step() function.  This can be done using code like

step(initial_model, scope = . ~ .^2, direction = 'forward')

where initial_model is the output object from lm().  Using the output object for a first-order model would be a good initial model.

Higher order interactions can also be explored by replacing the 2 with the highest level of interaction desired, but we won't go there in this assignment.

Fit the model from the forward step done previously

racetime~estffm + expvarsity + tall + preexper + biceps + meso + calfcir + bestvj

and use step() to look for the best model containing up to any two-way interaction terms.  Report the model and the corresponding AIC for it.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1h -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #storing the results of the forward step() model into a variable
model = lm(racetime~estffm + expvarsity + tall + preexper + biceps + meso + calfcir + bestvj, data = rowtime)
  #using that model as a starting point, check the combination of all interaction terms to see if they add additional value.
  #the first . means all variables. the * means interaction, and the second . means all teams. So to summarize, check all terms interactions with all terms. 
step(model, scope = .~.*., direction = "forward")
```

Using the forward step() function to test the combination of all term interactions, we have identified a better model with AIC of 491.18. This model is:

racetime ~ estffm + expvarsity + tall + preexper + biceps + meso + calfcir + bestvj + tall:calfcir + estffm:bestvj

### Part 1i

Obtain the model summary for the model that resulted in Part 1h. Are there any predictors with coefficients that do not have coefficients that differ from 0 at the 5% level? 

If so, drop those predictors from the model if they are not involved in any interactions and re-fit it without them.  Compute both the $R^2_{adj}$ and the AIC for that model.  

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1i -|-|-|-|-|-|-|-|-|-|-|-

```{r}
 #create a summary of the new model with the two additional interaction terms. 
model_f = lm(racetime~estffm+expvarsity+tall+preexper+biceps+meso+calfcir+bestvj+tall:calfcir+estffm:bestvj, data=rowtime)
summary(model_f)
```

Although the estffm variable is not significant on its own, it is considered significant when it is part of the interaction between estffm:bestvj, therefore i will not drop the individual term. This model results in staying the same, with an AIC of 491.18, and an Adjusted R2 of 0.6484.

### Part 1j 

Let us refer to this final model as **Model F**.  It should include the following terms:

racetime ~  estffm + expvarsity + tall + preexper + biceps + meso + calfcir + bestvj + tall:calfcir + estffm:bestvj

If this will be our possible final model, it is necessary to evaluate the model assumptions.  
    
Are the residuals of **Model F** normal?  Construct a histogram, normal probability plot, and boxplot of the residuals and perform a Shapiro-Wilk test for normality at a 5% level of significance. Justify your answer.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1j -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #create a histogram, qqplot, and boxplot of the residuals. 
hist(model_f$residuals, xlab="Residuals", ylab = "Count", main = "Distribution of Residuals for Model_F")
qqnorm(model_f$residuals, main = "QQ Plot Residuals of Model_F")
qqline(model_f$residuals)
boxplot(model_f$residuals, ylab = "Count", main = "Distribution of Residuals for Model_F")
  #check normality of the residuals with the shapiro test. 
shapiro.test(model_f$residuals)

```


After reviewing the graphs above and performing the Shapiro test for normality, the residuals of the fitted model_f appear to be normally distributed (shapiro's pvalue = 0.9911)


### Part 1k

Construct a residual plot for **Model F**. Do you see any patterns indicating potential violations of model assumptions?  Explain.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1k -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #create a residual plot of residuals vs. fitted values. 
plot(model_f$fitted.values, model_f$residuals, xlab="Fitted Values", ylab = "Residuals", main = "Residual Plot for Model_F")
```


There doesn't appear to be any visual patterns that may indicate any violations for the model's assumptions. Perhaps an outlier or two, but more investigation would need to be done.  


### Part 1l

Perform the Bruesch-Pagan test for equal variances of the residuals at a 5% level of significance.  What does you conclude from this test?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1l -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #load in the lmtest library
library(lmtest)
  #run the equal variance test.
bptest(model_f)
```

At a 95% level of significance, there is not enough evidence to suggest that model_f's residuals are not equal (p-value = 0.3222). 

### Part 1m

How do you feel about this last model being the "best"?

### -|-|-|-|-|-|-|-|-|-|-|- Answer 1m -|-|-|-|-|-|-|-|-|-|-|-

Given that all the assumptions check out, I feel pretty good about the model being the best as it stands today - the Adjusted R2 value on this final model is about 0.65 (thus this model should explain about 65% of the variability in NCAA D1 Women's row time performance) and the terms are not overly complicated or extraneous. 


## Exercise 2

In a study of small, constructed agricultural ponds in southeastern Minnesota, pond and the surrounding landscape features were used to assess their value as amphibian breeding sites. One measure of this was when the amphibian species richness was at least four.  

The data frame is farmpond.rda in the DS705data package.

Species richness is the number of different species observed at each pond and the variable RICH is defined as:

RICH = 1 if species richness is at least 4; RICH = 0 otherwise.

Furthermore,

FISH = 1 if fish are present; FISH = 0 otherwise

and the remaining variables are quantitative. 

### Part 2a

Suppose our goal is to build the "best" logistic regression model to predict species richness of at least 4 (i.e. RICH=1).  Fit the first order logistic regression model using all of the available predictors in the file.

Also fit the null model (intercept only) and use step() with forward selection to search for the best first-order logistic regression model with these variables.  Identify the resulting model.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2a -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #load the farmpond dataset and build a logistic regression model for the RICH variable using all the other predictor variables.
data("farmpond")
  #building a full model (for the upper limit)
full2model = glm(RICH~., data = farmpond, family = "binomial")
  #building a null model (or starting point, for the lower bounds) so the "step forward" can add to it.
null2model = glm(RICH~1, data = farmpond, family = "binomial")
  #starting with the null model, add one variable at a time until you bottom out on the AIC score. 
  #dont forget make the lower and upper a LIST type.
step(null2model, scope = list(lower = null2model, upper = full2model), direction = "forward")
```

The "best" model with an AIC score of 33.59 is:

RICH ~ COND + TOTNITR + FISH

### Part 2b

Construct a classification table (also known as a confusion matrix) for the model identified in the previous part.  Use 0.5 as the cutoff probability.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2b -|-|-|-|-|-|-|-|-|-|-|-

```{r echo=FALSE}
model2 = glm(RICH~ COND + TOTNITR + FISH, data = farmpond, family = "binomial")
predprob <- fitted(model2) # get predicted probabilities

threshhold <- 0.5  # Set Y=1 when predicted probability exceeds this
predRICH <- cut(predprob, breaks=c(-Inf, threshhold, Inf), 
                labels=c("Sp Rich<4", "Sp Rich>=4"))  # Y=1 is "Sp Rich>=4" here

cTab <- table(farmpond$RICH, predRICH) 
addmargins(cTab)

p <- sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
print(paste('Proportion correctly predicted = ', p)) 

```

### Part 2c

Compute McFadden's pseudo $R^2$ for the model identified in part 2a.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2c -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #load the function from the pscl package
library(pscl)
  #using the pR2 function to get the "McFaddens pseudo R2" value. This is like an R2 value for linear regression, but for logistic.
pR2(model2)[4]

```


### Part 2d

Conduct a Hosmer-Lemeshow test of goodness-of-fit for the model from part 2a.  Use 5 groups and use a 5% level of significance.

### -|-|-|-|-|-|-|-|-|-|-|- Answer 2d -|-|-|-|-|-|-|-|-|-|-|-

```{r}
  #load the function from the ResourceSelection package
library(ResourceSelection)
  #conduct the Hoslem-Lemeshow GOF test for logistic regression. This basically tests if the model is a good fit to the data.
  #null = is a good fit; alt = is not a good fit.
hoslem.test(farmpond$RICH, fitted(model2), g=5)
```

At a 95% level of significance, there is not enough evidence to suggest that the model is not a good fit to the data (p-value = 0.8252).




