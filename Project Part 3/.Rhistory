hist(log(loans$totalRevLim), main = "log(totalRevLim)", xlab = "")
hist(loans$bcOpen, main = "bcOpen", xlab = "")
hist(log(loans$bcOpen), main = "log(bcOpen)", xlab = "")
#Line 3: totalLim & total RevBal
hist(loans$totalLim, main = "totalLim", xlab = "")
hist(log(loans$totalLim), main = "log(totalLim)", xlab = "")
hist(loans$totalRevBal, main = "totalRevBal", xlab = "")
hist(log(loans$totalRevBal), main = "log(totalRevBal)", xlab = "")
#Line 4: totalBcLim & totalILim
hist(loans$totalBcLim, main = "totalBcLim", xlab = "")
hist(log(loans$totalBcLim), main = "log(totalBcLim)", xlab = "")
hist(loans$totalIlLim, main = "totalIlLim", xlab = "")
hist(log(loans$totalIlLim), main = "log(totalIlLim)", xlab = "")
#Line 5: avgBal
hist(loans$avgBal, main = "avgBal", xlab = "")
hist(log(loans$avgBal), main = "log(avgBal)", xlab = "")
#reset my graphs back to a 1x1.
par(mfrow=c(1,1))
#creating a list of columns to transform
cols_to_transform = c("totalBal","totalRevLim","bcOpen","totalLim","totalRevBal","totalBcLim","totalIlLim","avgBal")
#because log(0) = -inf, add +1 to the values first just to ensure this doesnt occur. It's by such a small factor that it wont be a large impact on the results.
loans[cols_to_transform] = loans[cols_to_transform]+1
#applying the log transform to the columns.
loans[cols_to_transform] = lapply(loans[cols_to_transform], log)
#remove variables - keep things tidy.
rm(cols_to_transform)
#create a side by side box plot that shows the distribution of loan amounts by the outcome
bx_amount=gf_boxplot(amount~outcome, data=loans, color=~outcome, xlab="Loan Outcome", ylab="Loan Amount ($)", title='Distribution of Loan "Amount"')
#create a side by side box plot that shows the distribution of loan amounts by the outcome
bx_rate=gf_boxplot(rate~outcome, data=loans, color=~outcome, xlab="Loan Outcome", ylab="Loan Rate (%)", title='Distribution of Loan "Rate"')
#use ggarrange to set ggplots next to each other
ggarrange(bx_amount,bx_rate, ncol = 2, nrow = 1)
#keeping things tidy
rm(bx_amount,bx_rate)
#test if the median dollar amount of loans in the bad loan group is greater than the median dollar amount of loans in the good loan group.
#p1 = The median dollar amount of loans in the bad loan group
#p2 = The median dollar amount of loans in the good loan group
#H0: p1 <= p2
#H_a: p1 > p2
#alpha = 0.05
wilcox.test(loans$amount~loans$outcome, alternative = "greater", conf.int = TRUE)
#test if the median of percentage rates in the bad loan group is greater than the median of percentage rates in the good loan group.
#p1 = The median rate in the bad loan group
#p2 = The median rate in the good loan group
#H0: p1 <= p2
#H_a: p1 > p2
#alpha = 0.05
wilcox.test(loans$rate~loans$outcome, alternative = "greater", conf.int = TRUE)
#create a side by side bar chart to see the distribution of loan term by outcome.
bar_grade=gf_bar(~grade, data = loans, fill = ~outcome, position = position_dodge(), title = 'Distribution of Loan "Grade"')
#create a side by side bar chart to see the distribution of loan term by outcome.
bar_term=gf_bar(~term, data = loans, fill = ~outcome, position = position_dodge(), title = 'Distribution of Loan "Term"')
#use ggarrange to set ggplots next to each other
ggarrange(bar_grade, bar_term, ncol = 2, nrow = 1)
#keeping things tidy
rm(bar_grade,bar_term)
#test if the number of "bad" loans is equal across each loan grade, or not equal to each other.
#p1 = The proportion of bad outcomes for loan grade "A"
#p2 = The proportion of bad outcomes for loan grade "B"
#p3 = The proportion of bad outcomes for loan grade "C"
#p4 = The proportion of bad outcomes for loan grade "D"
#p5 = The proportion of bad outcomes for loan grade "E or less"
#H0: p1 = p2 = p3 = p4 = p5
#H_a: p1 <> p2 <> p3 <> p4 <> p5
#alpha = 0.05
#pull the counts of bad loans per each loan grade.
bad_A = nrow(loans[loans$outcome == "Bad" & loans$grade == "A",])
bad_B = nrow(loans[loans$outcome == "Bad" & loans$grade == "B",])
bad_C = nrow(loans[loans$outcome == "Bad" & loans$grade == "C",])
bad_D = nrow(loans[loans$outcome == "Bad" & loans$grade == "D",])
bad_E = nrow(loans[loans$outcome == "Bad" & loans$grade == "E or less",])
#create a matrix of grade, count, and equal proportions
grades = matrix(c("A",bad_A,0.20,"B",bad_B,0.20,"C",bad_C,0.20,"D",bad_D,0.20,"E or Less",bad_E,0.20), ncol = 3, byrow = TRUE)
colnames(grades) = c("Grade","Frequency of Bad","equaldistribution")
#run the chi-squared goodness of fit test
grades_test = chisq.test(as.numeric(grades[,2]), p = as.numeric(grades[,3]))
#check expected values -> they are greater than 5 per category.
grades_test$expected
#review results
grades_test
#keeping things tidy
rm(grades,grades_test,bad_A,bad_B,bad_C,bad_D,bad_E)
#test if the proportion of bad loans in 36 month term is less than the proportion of bad loans in the 60 month term.
#p1 = The proportion of bad loans in the 36 month term
#p2 = The proportion of bad loans in the 60 month term
#H0: p1 >= p2
#H_a: p1 < p2
#alpha = 0.05
#get the number of bad records from 36 month term
bad_36 = nrow(loans[loans$outcome == "Bad" & loans$term == "36 months",])
#get the number of good records from 36 month term
good_36 = nrow(loans[loans$outcome == "Good" & loans$term == "36 months",])
#get the number of bad records from 36 month term
bad_60 = nrow(loans[loans$outcome == "Bad" & loans$term == "60 months",])
#get the number of good records from 36 month term
good_60 = nrow(loans[loans$outcome == "Good" & loans$term == "60 months",])
#create a matrix of bad/good for 36/60 months
term = matrix(c(bad_36,good_36,bad_60,good_60), ncol = 2, byrow = TRUE)
rownames(term) = c("36 months","60 months")
colnames(term) = c("Bad","Good")
#data is prepped, so run prop.test to check if: proportion of bad loans in the 36month term < proportion of bad loans in the 60month term.
prop.test(term, alternative = "less", correct = TRUE)
#keeping things tidy
rm(bad_36,bad_60,good_36,good_60,term)
set.seed(123)
#randomly select 80% of the rows from the dataset.
training_data = sample(seq_len(nrow(loans)),size = floor(0.80*nrow(loans)))
#the 80% selected go into the train dataset
train =loans[training_data,]
#the remaining rows go into the test dataset
test =loans[-training_data,]
#keeping things tidy
rm(training_data)
fullmodel = glm(outcome~., data = train, family = "binomial")
vif(fullmodel)
#create a full model without the totalBal since it had the highest VIF score
fullmodel = glm(outcome~., data = subset(train, select = -c(totalBal)), family = "binomial")
vif(fullmodel)
#create a full model without the totalBal + amount since it had the next highest VIF score
fullmodel = glm(outcome~., data = subset(train, select = -c(totalBal, amount)), family = "binomial")
vif(fullmodel)
#create a full model without the totalBal + amount + totalLim since it had the next highest VIF score
fullmodel = glm(outcome~., data = subset(train, select = -c(totalBal, amount, totalLim)), family = "binomial")
vif(fullmodel) #there are no other VIF score > 10, so proceed.
nullmodel = glm(outcome~1, data = train, family = "binomial")
step(nullmodel, scope = list(lower = nullmodel, upper = fullmodel), direction = "forward")
#results of the forward step regression results in this model.
model = glm(formula = outcome ~ grade + term + accOpen24 + home + income +
payment + bcOpen + delinq2yr + avgBal + totalIlLim + rate +
totalRevLim + totalRevBal + inq6mth, family = "binomial",
data = train)
#check significance of variables selected.
summary(model)
#totalIlLim + inq6mth are not significant, therefore are removed.
model = glm(formula = outcome ~ grade + term + accOpen24 + home + income +
payment + bcOpen + delinq2yr + avgBal + rate +
totalRevLim + totalRevBal, family = "binomial",
data = train)
summary(model)
#keeping things tidy
rm(nullmodel,fullmodel)
predprob = fitted(model) # get predicted probabilities
threshold = 0.5  # Set Y=1 when predicted probability exceeds this value.
predGood = cut(predprob, breaks=c(-Inf, threshold, Inf),
labels=c("Bad", "Good"))  # Y=1 is "Good"
cTab = table(train$outcome, predGood)
addmargins(cTab)
p = sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
print(paste('Proportion correctly predicted = ', round(p,2)))
predprob = predict(model,test, type = "response") # get predicted probabilities
threshold = 0.5  # Set Y=1 when predicted probability exceeds this value.
predGood = cut(predprob, breaks=c(-Inf, threshold, Inf),
labels=c("Bad", "Good"))  # Y=1 is "Good"
cTab = table(test$outcome, predGood)
addmargins(cTab)
p = sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
print(paste('Proportion correctly predicted = ', round(p,2)))
#keeping things tidy
rm(threshold, predGood,p)
#"bad" accuracy ~9%
round(addmargins(cTab)[1]/addmargins(cTab)[7],2)
#"good" accuracy ~98%
round(addmargins(cTab)[5]/addmargins(cTab)[8],2)
#keeping things tidy
rm(cTab)
calc_overall_accuracy <- function(probabilties, threshold){
predGood = cut(probabilties, breaks=c(-Inf, threshold, Inf),labels=c("Bad", "Good"))
cTab = table(test$outcome, predGood)
addmargins(cTab)
overall_acc = round(sum(diag(cTab))/sum(cTab),2)
return(list(threshold,overall_acc,"overall"))
}
#this function accepts the predicted probabilities and a threshold value to evaluate.
#it returns the accuracy calculation, the threshold that was passed, and the tag "good" since it calculates for the good outcome.
calc_good_accuracy <- function(probabilties, threshold){
#using the threshold passed and the set of probabilities, set anything above threshold to 1, else 0.
predGood = cut(probabilties, breaks=c(-Inf, threshold, Inf),labels=c("Bad", "Good"))
#build a contingency table and add the sum margins
cTab = table(test$outcome, predGood)
addmargins(cTab)
#calculate the accuracy using the summarized values
good_acc = round(addmargins(cTab)[5]/addmargins(cTab)[8],2)
#return the row
return(list(threshold,good_acc,"good"))
}
#this function accepts the predicted probabilities and a threshold value to evaluate.
#it returns the accuracy calculation, the threshold that was passed, and the tag "bad" since it calculates for the good outcome.
calc_bad_accuracy <- function(probabilties, threshold){
#using the threshold passed and the set of probabilities, set anything above threshold to 1, else 0.
predGood = cut(probabilties, breaks=c(-Inf, threshold, Inf),labels=c("Bad", "Good"))
#build a contingency table and add the sum margins
cTab = table(test$outcome, predGood)
addmargins(cTab)
#calculate the accuracy using the summarized values
bad_acc = round(addmargins(cTab)[1]/addmargins(cTab)[7],2)
#return the row
return(list(threshold,bad_acc,"bad"))
}
#create an empty dataframe of 3 columns
accuracy_tradeoff = data.frame(matrix(ncol = 3, nrow = 0))
#populate the dataframe with accuracy calculations with threshold values ranging from 0.01 to 1.00.
for (index in seq(0.00, 1.00, 0.01)){
accuracy_tradeoff = rbind(accuracy_tradeoff, calc_overall_accuracy(predprob,index))
accuracy_tradeoff = rbind(accuracy_tradeoff, calc_good_accuracy(predprob,index))
accuracy_tradeoff = rbind(accuracy_tradeoff, calc_bad_accuracy(predprob,index))
}
#change column headers for ease of use later on.
accuracy_tradeoff = setNames(accuracy_tradeoff, c("threshold", "accuracy","outcome"))
#look at the first 6 rows.
head(accuracy_tradeoff)
#keeping things tidy
rm(index)
#pull out good/bad accuracies into their own dataframe this will be used to set the black vertical and horizontal lines, which pinpoint the optimal threshold value.
good=accuracy_tradeoff[accuracy_tradeoff$outcome == "good",]
bad=accuracy_tradeoff[accuracy_tradeoff$outcome == "bad",]
#create the line plot with accuracy as a function of the threshold, colored by the outcome in the dataframe. Add intercept lines.
gf_line(accuracy~threshold, data = accuracy_tradeoff, color = ~outcome) %>%
gf_labs(x="Threshold", y="Accuracy", title= "Accuracy Curves") %>%
#hline @0.66
gf_hline(color = "black", yintercept = ~ good$accuracy[which.min(abs(good$accuracy-bad$accuracy))]) %>%
#vline @0.78
gf_vline(color = "black", xintercept = ~ good$threshold[which.min(abs(good$accuracy-bad$accuracy))])
#keeping things tidy
rm(bad,good)
#pull out good/bad accuracies into their own dataframe this will be used to set the black vertical and horizontal lines, which pinpoint the optimal threshold value.
good=accuracy_tradeoff[accuracy_tradeoff$outcome == "good",]
bad=accuracy_tradeoff[accuracy_tradeoff$outcome == "bad",]
#create the line plot with accuracy as a function of the threshold, colored by the outcome in the dataframe. Add intercept lines.
gf_line(accuracy~threshold, data = accuracy_tradeoff, color = ~outcome) %>%
gf_labs(x="Threshold", y="Accuracy", title= "Accuracy Curves") %>%
#hline @0.66
gf_hline(color = "black", yintercept = ~ good$accuracy[which.min(abs(good$accuracy-bad$accuracy))]) %>%
#vline @0.78
gf_vline(color = "black", xintercept = ~ good$threshold[which.min(abs(good$accuracy-bad$accuracy))])
#keeping things tidy
rm(bad,good)
knitr::opts_chunk$set(fig.width = 4, fig.height = 4)
lapply(c("readr","dplyr","car","mice","ggformula","ggpubr"), require, character.only = TRUE)
#load the data
loans = read_csv("loans50k.csv")
loans = loans[loans$status %in% c("Fully Paid", "Charged Off", "Default"),]
loans = loans %>% mutate(outcome = as.factor(if_else(status == "Fully Paid","Good","Bad")))
loans = subset(loans, select = -status)
loans = subset(loans, select = -c(loanID,employment,verified,state,debtIncRat,revolRatio,totalAcc,totalPaid,bcRatio))
str(loans)
loans = mutate_at(loans, vars(term,grade,length,home,reason), as.factor)
par(mfrow=c(2,3),mar=c(4,3,3,1))
barplot(table(loans$term), main = "Loan Term")
barplot(table(loans$grade), main = "Loan Grade")
barplot(table(loans$length), las=2, main = "Employment Length")
barplot(table(loans$home), main = "Living Type")
barplot(table(loans$reason), las=2, cex.names = 0.9, main = "Loan Reason")
loans$grade = recode(loans$grade,"c('E','F','G')='E or less'")
loans$length = recode(loans$length,"c('< 1 year','1 year','2 years','3 years','4 years')='0-4 years';c('5 years','6 years','7 years','8 years','9 years')='5-9 years'")
loans$reason = recode(loans$reason,"'credit_card'='Credit Card';'debt_consolidation'='Debt Consolidation'; c('home_improvement','house','moving','renewable_energy')='Home Expense'; c('car','major_purchase','medical','other','small_business','vacation','wedding')='Other'")
loans = loans[which(loans$length != "n/a"),]
impute_bcOpen <- function(df, seed){
#create a vector of the indices that are "NA"
index_NA = which(is.na(df$bcOpen))
#complete the imputation process
imputation = mice(loans, seed = seed)
#iterate through each missing value and calculate the average of the imputed set, replace NA with the imputed value.
for(i in 1:length(index_NA)){
sum = 0
sum = sum + imputation$imp$bcOpen$`1`[i]
sum = sum + imputation$imp$bcOpen$`2`[i]
sum = sum + imputation$imp$bcOpen$`3`[i]
sum = sum + imputation$imp$bcOpen$`4`[i]
sum = sum + imputation$imp$bcOpen$`5`[i]
df$bcOpen[index_NA[i]] = sum/5
}
return(df)
}
loans = impute_bcOpen(loans, seed = 123456)
par(mfrow=c(1,2),mar=c(4,4,4,1))
hist(loans$income, main = "Distribution of Income", xlab = "Dollars ($)")
hist(log(loans$income), main = "Distribution of log(Income)", xlab = "Dollars (Log $)")
par(mfrow=c(1,1))
loans$income = log(loans$income)
#set up 4 rows of 4 columns. Give enough margin spacing with mar()
par(mfrow=c(5,4),mar=c(4,4,4,1))
#Line 1: amount & totalBal
hist(loans$amount, main = "amount", xlab = "")
hist(log(loans$amount), main = "log(amount)", xlab = "")
hist(loans$totalBal, main = "totalBal", xlab = "")
hist(log(loans$totalBal), main = "log(totalBal)", xlab = "")
#Line 2: totalRevLim & bcOpen
hist(loans$totalRevLim, main = "totalRevLim", xlab = "")
hist(log(loans$totalRevLim), main = "log(totalRevLim)", xlab = "")
hist(loans$bcOpen, main = "bcOpen", xlab = "")
hist(log(loans$bcOpen), main = "log(bcOpen)", xlab = "")
#Line 3: totalLim & total RevBal
hist(loans$totalLim, main = "totalLim", xlab = "")
hist(log(loans$totalLim), main = "log(totalLim)", xlab = "")
hist(loans$totalRevBal, main = "totalRevBal", xlab = "")
hist(log(loans$totalRevBal), main = "log(totalRevBal)", xlab = "")
#Line 4: totalBcLim & totalILim
hist(loans$totalBcLim, main = "totalBcLim", xlab = "")
hist(log(loans$totalBcLim), main = "log(totalBcLim)", xlab = "")
hist(loans$totalIlLim, main = "totalIlLim", xlab = "")
hist(log(loans$totalIlLim), main = "log(totalIlLim)", xlab = "")
#Line 5: avgBal
hist(loans$avgBal, main = "avgBal", xlab = "")
hist(log(loans$avgBal), main = "log(avgBal)", xlab = "")
#reset my graphs back to a 1x1.
par(mfrow=c(1,1))
#creating a list of columns to transform
cols_to_transform = c("totalBal","totalRevLim","bcOpen","totalLim","totalRevBal","totalBcLim","totalIlLim","avgBal")
#because log(0) = -inf, add +1 to the values first just to ensure this doesnt occur. It's by such a small factor that it wont be a large impact on the results.
loans[cols_to_transform] = loans[cols_to_transform]+1
#applying the log transform to the columns.
loans[cols_to_transform] = lapply(loans[cols_to_transform], log)
#remove variables - keep things tidy.
rm(cols_to_transform)
#create a side by side box plot that shows the distribution of loan amounts by the outcome
bx_amount=gf_boxplot(amount~outcome, data=loans, color=~outcome, xlab="Loan Outcome", ylab="Loan Amount ($)", title='Distribution of Loan "Amount"')
#create a side by side box plot that shows the distribution of loan amounts by the outcome
bx_rate=gf_boxplot(rate~outcome, data=loans, color=~outcome, xlab="Loan Outcome", ylab="Loan Rate (%)", title='Distribution of Loan "Rate"')
#use ggarrange to set ggplots next to each other
ggarrange(bx_amount,bx_rate, ncol = 2, nrow = 1)
#keeping things tidy
rm(bx_amount,bx_rate)
#test if the median dollar amount of loans in the bad loan group is greater than the median dollar amount of loans in the good loan group.
#p1 = The median dollar amount of loans in the bad loan group
#p2 = The median dollar amount of loans in the good loan group
#H0: p1 <= p2
#H_a: p1 > p2
#alpha = 0.05
wilcox.test(loans$amount~loans$outcome, alternative = "greater", conf.int = TRUE)
#test if the median of percentage rates in the bad loan group is greater than the median of percentage rates in the good loan group.
#p1 = The median rate in the bad loan group
#p2 = The median rate in the good loan group
#H0: p1 <= p2
#H_a: p1 > p2
#alpha = 0.05
wilcox.test(loans$rate~loans$outcome, alternative = "greater", conf.int = TRUE)
#create a side by side bar chart to see the distribution of loan term by outcome.
bar_grade=gf_bar(~grade, data = loans, fill = ~outcome, position = position_dodge(), title = 'Distribution of Loan "Grade"')
#create a side by side bar chart to see the distribution of loan term by outcome.
bar_term=gf_bar(~term, data = loans, fill = ~outcome, position = position_dodge(), title = 'Distribution of Loan "Term"')
#use ggarrange to set ggplots next to each other
ggarrange(bar_grade, bar_term, ncol = 2, nrow = 1)
#keeping things tidy
rm(bar_grade,bar_term)
#test if the number of "bad" loans is equal across each loan grade, or not equal to each other.
#p1 = The proportion of bad outcomes for loan grade "A"
#p2 = The proportion of bad outcomes for loan grade "B"
#p3 = The proportion of bad outcomes for loan grade "C"
#p4 = The proportion of bad outcomes for loan grade "D"
#p5 = The proportion of bad outcomes for loan grade "E or less"
#H0: p1 = p2 = p3 = p4 = p5
#H_a: p1 <> p2 <> p3 <> p4 <> p5
#alpha = 0.05
#pull the counts of bad loans per each loan grade.
bad_A = nrow(loans[loans$outcome == "Bad" & loans$grade == "A",])
bad_B = nrow(loans[loans$outcome == "Bad" & loans$grade == "B",])
bad_C = nrow(loans[loans$outcome == "Bad" & loans$grade == "C",])
bad_D = nrow(loans[loans$outcome == "Bad" & loans$grade == "D",])
bad_E = nrow(loans[loans$outcome == "Bad" & loans$grade == "E or less",])
#create a matrix of grade, count, and equal proportions
grades = matrix(c("A",bad_A,0.20,"B",bad_B,0.20,"C",bad_C,0.20,"D",bad_D,0.20,"E or Less",bad_E,0.20), ncol = 3, byrow = TRUE)
colnames(grades) = c("Grade","Frequency of Bad","equaldistribution")
#run the chi-squared goodness of fit test
grades_test = chisq.test(as.numeric(grades[,2]), p = as.numeric(grades[,3]))
#check expected values -> they are greater than 5 per category.
grades_test$expected
#review results
grades_test
#keeping things tidy
rm(grades,grades_test,bad_A,bad_B,bad_C,bad_D,bad_E)
#test if the proportion of bad loans in 36 month term is less than the proportion of bad loans in the 60 month term.
#p1 = The proportion of bad loans in the 36 month term
#p2 = The proportion of bad loans in the 60 month term
#H0: p1 >= p2
#H_a: p1 < p2
#alpha = 0.05
#get the number of bad records from 36 month term
bad_36 = nrow(loans[loans$outcome == "Bad" & loans$term == "36 months",])
#get the number of good records from 36 month term
good_36 = nrow(loans[loans$outcome == "Good" & loans$term == "36 months",])
#get the number of bad records from 36 month term
bad_60 = nrow(loans[loans$outcome == "Bad" & loans$term == "60 months",])
#get the number of good records from 36 month term
good_60 = nrow(loans[loans$outcome == "Good" & loans$term == "60 months",])
#create a matrix of bad/good for 36/60 months
term = matrix(c(bad_36,good_36,bad_60,good_60), ncol = 2, byrow = TRUE)
rownames(term) = c("36 months","60 months")
colnames(term) = c("Bad","Good")
#data is prepped, so run prop.test to check if: proportion of bad loans in the 36month term < proportion of bad loans in the 60month term.
prop.test(term, alternative = "less", correct = TRUE)
#keeping things tidy
rm(bad_36,bad_60,good_36,good_60,term)
set.seed(123)
#randomly select 80% of the rows from the dataset.
training_data = sample(seq_len(nrow(loans)),size = floor(0.80*nrow(loans)))
#the 80% selected go into the train dataset
train =loans[training_data,]
#the remaining rows go into the test dataset
test =loans[-training_data,]
#keeping things tidy
rm(training_data)
fullmodel = glm(outcome~., data = train, family = "binomial")
vif(fullmodel)
#create a full model without the totalBal since it had the highest VIF score
fullmodel = glm(outcome~., data = subset(train, select = -c(totalBal)), family = "binomial")
vif(fullmodel)
#create a full model without the totalBal + amount since it had the next highest VIF score
fullmodel = glm(outcome~., data = subset(train, select = -c(totalBal, amount)), family = "binomial")
vif(fullmodel)
#create a full model without the totalBal + amount + totalLim since it had the next highest VIF score
fullmodel = glm(outcome~., data = subset(train, select = -c(totalBal, amount, totalLim)), family = "binomial")
vif(fullmodel) #there are no other VIF score > 10, so proceed.
nullmodel = glm(outcome~1, data = train, family = "binomial")
step(nullmodel, scope = list(lower = nullmodel, upper = fullmodel), direction = "forward")
#results of the forward step regression results in this model.
model = glm(formula = outcome ~ grade + term + accOpen24 + home + income +
payment + bcOpen + delinq2yr + avgBal + totalIlLim + rate +
totalRevLim + totalRevBal + inq6mth, family = "binomial",
data = train)
#check significance of variables selected.
summary(model)
#totalIlLim + inq6mth are not significant, therefore are removed.
model = glm(formula = outcome ~ grade + term + accOpen24 + home + income +
payment + bcOpen + delinq2yr + avgBal + rate +
totalRevLim + totalRevBal, family = "binomial",
data = train)
summary(model)
#keeping things tidy
rm(nullmodel,fullmodel)
predprob = fitted(model) # get predicted probabilities
threshold = 0.5  # Set Y=1 when predicted probability exceeds this value.
predGood = cut(predprob, breaks=c(-Inf, threshold, Inf),
labels=c("Bad", "Good"))  # Y=1 is "Good"
cTab = table(train$outcome, predGood)
addmargins(cTab)
p = sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
print(paste('Proportion correctly predicted = ', round(p,2)))
predprob = predict(model,test, type = "response") # get predicted probabilities
threshold = 0.5  # Set Y=1 when predicted probability exceeds this value.
predGood = cut(predprob, breaks=c(-Inf, threshold, Inf),
labels=c("Bad", "Good"))  # Y=1 is "Good"
cTab = table(test$outcome, predGood)
addmargins(cTab)
p = sum(diag(cTab)) / sum(cTab)  # compute the proportion of correct classifications
print(paste('Proportion correctly predicted = ', round(p,2)))
#keeping things tidy
rm(threshold, predGood,p)
#"bad" accuracy ~9%
round(addmargins(cTab)[1]/addmargins(cTab)[7],2)
#"good" accuracy ~98%
round(addmargins(cTab)[5]/addmargins(cTab)[8],2)
#keeping things tidy
rm(cTab)
calc_overall_accuracy <- function(probabilties, threshold){
predGood = cut(probabilties, breaks=c(-Inf, threshold, Inf),labels=c("Bad", "Good"))
cTab = table(test$outcome, predGood)
addmargins(cTab)
overall_acc = round(sum(diag(cTab))/sum(cTab),2)
return(list(threshold,overall_acc,"overall"))
}
#this function accepts the predicted probabilities and a threshold value to evaluate.
#it returns the accuracy calculation, the threshold that was passed, and the tag "good" since it calculates for the good outcome.
calc_good_accuracy <- function(probabilties, threshold){
#using the threshold passed and the set of probabilities, set anything above threshold to 1, else 0.
predGood = cut(probabilties, breaks=c(-Inf, threshold, Inf),labels=c("Bad", "Good"))
#build a contingency table and add the sum margins
cTab = table(test$outcome, predGood)
addmargins(cTab)
#calculate the accuracy using the summarized values
good_acc = round(addmargins(cTab)[5]/addmargins(cTab)[8],2)
#return the row
return(list(threshold,good_acc,"good"))
}
#this function accepts the predicted probabilities and a threshold value to evaluate.
#it returns the accuracy calculation, the threshold that was passed, and the tag "bad" since it calculates for the good outcome.
calc_bad_accuracy <- function(probabilties, threshold){
#using the threshold passed and the set of probabilities, set anything above threshold to 1, else 0.
predGood = cut(probabilties, breaks=c(-Inf, threshold, Inf),labels=c("Bad", "Good"))
#build a contingency table and add the sum margins
cTab = table(test$outcome, predGood)
addmargins(cTab)
#calculate the accuracy using the summarized values
bad_acc = round(addmargins(cTab)[1]/addmargins(cTab)[7],2)
#return the row
return(list(threshold,bad_acc,"bad"))
}
#create an empty dataframe of 3 columns
accuracy_tradeoff = data.frame(matrix(ncol = 3, nrow = 0))
#populate the dataframe with accuracy calculations with threshold values ranging from 0.01 to 1.00.
for (index in seq(0.00, 1.00, 0.01)){
accuracy_tradeoff = rbind(accuracy_tradeoff, calc_overall_accuracy(predprob,index))
accuracy_tradeoff = rbind(accuracy_tradeoff, calc_good_accuracy(predprob,index))
accuracy_tradeoff = rbind(accuracy_tradeoff, calc_bad_accuracy(predprob,index))
}
#change column headers for ease of use later on.
accuracy_tradeoff = setNames(accuracy_tradeoff, c("threshold", "accuracy","outcome"))
#look at the first 6 rows.
head(accuracy_tradeoff)
#keeping things tidy
rm(index)
#pull out good/bad accuracies into their own dataframe this will be used to set the black vertical and horizontal lines, which pinpoint the optimal threshold value.
good=accuracy_tradeoff[accuracy_tradeoff$outcome == "good",]
bad=accuracy_tradeoff[accuracy_tradeoff$outcome == "bad",]
#create the line plot with accuracy as a function of the threshold, colored by the outcome in the dataframe. Add intercept lines.
gf_line(accuracy~threshold, data = accuracy_tradeoff, color = ~outcome) %>%
gf_labs(x="Threshold", y="Accuracy", title= "Accuracy Curves") %>%
#hline @0.66
gf_hline(color = "black", yintercept = ~ good$accuracy[which.min(abs(good$accuracy-bad$accuracy))]) %>%
#vline @0.78
gf_vline(color = "black", xintercept = ~ good$threshold[which.min(abs(good$accuracy-bad$accuracy))])
#keeping things tidy
rm(bad,good)
